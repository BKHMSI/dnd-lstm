{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demo: training a DND LSTM on a contextual choice task\n",
    "\n",
    "This is an implementation of the following paper: \n",
    "```\n",
    "Ritter, S., Wang, J. X., Kurth-Nelson, Z., Jayakumar, S. M., Blundell, C., Pascanu, R., & Botvinick, M. (2018).  \n",
    "Been There, Done That: Meta-Learning with Episodic Recall. arXiv [stat.ML].  \n",
    "Retrieved from http://arxiv.org/abs/1805.09692\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you are using google colab, uncomment and run the following lines!  \\nwhich grabs the dependencies from github\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are using google colab, uncomment and run the following lines!  \n",
    "which grabs the dependencies from github\n",
    "'''\n",
    "# !git clone https://github.com/qihongl/dnd-lstm.git\n",
    "# !cd dnd-lstm/src/\n",
    "# import os\n",
    "# os.chdir('dnd-lstm/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from task import ContextualChoice\n",
    "from model import DNDLSTM as Agent\n",
    "from utils import compute_stats, to_sqnp\n",
    "from model.DND import compute_similarities\n",
    "from model.utils import get_reward, compute_returns, compute_a2c_loss\n",
    "\n",
    "sns.set(style='white', context='talk', palette='colorblind')\n",
    "seed_val = 0\n",
    "torch.manual_seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''init task'''\n",
    "n_unique_example = 50\n",
    "n_trials = 2 * n_unique_example\n",
    "# n time steps of a trial\n",
    "trial_length = 10\n",
    "# after `tp_corrupt`, turn off the noise\n",
    "t_noise_off = 5\n",
    "# input/output/hidden/memory dim\n",
    "obs_dim = 32\n",
    "task = ContextualChoice(\n",
    "    obs_dim=obs_dim, trial_length=trial_length,\n",
    "    t_noise_off=t_noise_off\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''init model'''\n",
    "# set params\n",
    "dim_hidden = 32\n",
    "dim_output = 2\n",
    "dict_len = 100\n",
    "learning_rate = 5e-4\n",
    "n_epochs = 20\n",
    "# init agent / optimizer\n",
    "agent = Agent(task.x_dim, dim_hidden, dim_output, dict_len)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | return = 5.86 | loss: val = 1.82, pol = 1.12 | time = 2.23\n",
      "Epoch   1 | return = 7.45 | loss: val = 1.06, pol = -0.64 | time = 2.48\n",
      "Epoch   2 | return = 7.85 | loss: val = 0.89, pol = -0.64 | time = 2.29\n",
      "Epoch   3 | return = 7.72 | loss: val = 0.83, pol = -0.75 | time = 2.25\n",
      "Epoch   4 | return = 7.64 | loss: val = 0.83, pol = -0.41 | time = 2.28\n",
      "Epoch   5 | return = 7.78 | loss: val = 0.79, pol = -0.31 | time = 2.26\n",
      "Epoch   6 | return = 8.27 | loss: val = 0.71, pol = -0.36 | time = 2.21\n",
      "Epoch   7 | return = 8.13 | loss: val = 0.76, pol = -0.33 | time = 2.32\n",
      "Epoch   8 | return = 8.15 | loss: val = 0.65, pol = -0.09 | time = 2.35\n",
      "Epoch   9 | return = 8.08 | loss: val = 0.71, pol = -0.28 | time = 2.24\n"
     ]
    }
   ],
   "source": [
    "'''train'''\n",
    "log_return = np.zeros(n_epochs,)\n",
    "log_loss_value = np.zeros(n_epochs,)\n",
    "log_loss_policy = np.zeros(n_epochs,)\n",
    "\n",
    "log_Y = np.zeros((n_epochs, n_trials, trial_length))\n",
    "log_Y_hat = np.zeros((n_epochs, n_trials, trial_length))\n",
    "\n",
    "# loop over epoch\n",
    "for i in range(n_epochs):\n",
    "    time_start = time.time()\n",
    "    # get data for this epoch\n",
    "    X, Y = task.sample(n_unique_example)\n",
    "    # flush hippocampus\n",
    "    agent.reset_memory()\n",
    "    agent.turn_on_retrieval()\n",
    "\n",
    "    # loop over the training set\n",
    "    for m in range(n_trials):\n",
    "        # prealloc\n",
    "        cumulative_reward = 0\n",
    "        probs, rewards, values = [], [], []\n",
    "        h_t, c_t = agent.get_init_states()\n",
    "\n",
    "        # loop over time, for one training example\n",
    "        for t in range(trial_length):\n",
    "            # only save memory at the last time point\n",
    "            agent.turn_off_encoding()\n",
    "            if t == trial_length-1 and m < n_unique_example:\n",
    "                agent.turn_on_encoding()\n",
    "            # recurrent computation at time t\n",
    "            output_t, _ = agent(X[m][t].view(1, 1, -1), h_t, c_t)\n",
    "            a_t, prob_a_t, v_t, h_t, c_t = output_t\n",
    "            # compute immediate reward\n",
    "            r_t = get_reward(a_t, Y[m][t])\n",
    "            # log\n",
    "            probs.append(prob_a_t)\n",
    "            rewards.append(r_t)\n",
    "            values.append(v_t)\n",
    "            # log\n",
    "            cumulative_reward += r_t\n",
    "            log_Y_hat[i, m, t] = a_t.item()\n",
    "\n",
    "        returns = compute_returns(rewards)\n",
    "        loss_policy, loss_value = compute_a2c_loss(probs, values, returns)\n",
    "        loss = loss_policy + loss_value\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log\n",
    "        log_Y[i] = np.squeeze(Y.numpy())\n",
    "        log_return[i] += cumulative_reward / n_trials\n",
    "        log_loss_value[i] += loss_value.item() / n_trials\n",
    "        log_loss_policy[i] += loss_policy.item() / n_trials\n",
    "\n",
    "    # print out some stuff\n",
    "    time_end = time.time()\n",
    "    run_time = time_end - time_start\n",
    "    print(\n",
    "        'Epoch %3d | return = %.2f | loss: val = %.2f, pol = %.2f | time = %.2f' %\n",
    "        (i, log_return[i], log_loss_value[i], log_loss_policy[i], run_time)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''learning curve'''\n",
    "f, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axes[0].plot(log_return)\n",
    "axes[0].set_ylabel('Return')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[1].plot(log_loss_value)\n",
    "axes[1].set_ylabel('Value loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "sns.despine()\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''show behavior'''\n",
    "corrects = log_Y_hat[-1] == log_Y[-1]\n",
    "acc_mu_no_memory, acc_se_no_memory = compute_stats(\n",
    "    corrects[:n_unique_example])\n",
    "acc_mu_has_memory, acc_se_has_memory = compute_stats(\n",
    "    corrects[n_unique_example:])\n",
    "\n",
    "n_se = 2\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "ax.errorbar(range(trial_length), y=acc_mu_no_memory,\n",
    "            yerr=acc_se_no_memory * n_se, label='w/o memory')\n",
    "ax.errorbar(range(trial_length), y=acc_mu_has_memory,\n",
    "            yerr=acc_se_has_memory * n_se, label='w/  memory')\n",
    "ax.axvline(t_noise_off, label='turn off noise', color='grey', linestyle='--')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Correct rate')\n",
    "ax.set_title('Choice accuracy by condition')\n",
    "f.legend(frameon=False, bbox_to_anchor=(1, .6))\n",
    "sns.despine()\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''visualize keys and values'''\n",
    "keys, vals = agent.get_all_mems()\n",
    "n_mems = len(agent.dnd.keys)\n",
    "dmat_kk, dmat_vv = np.zeros((n_mems, n_mems)), np.zeros((n_mems, n_mems))\n",
    "for i in range(n_mems):\n",
    "    dmat_kk[i, :] = to_sqnp(compute_similarities(\n",
    "        keys[i], keys, agent.dnd.kernel))\n",
    "    dmat_vv[i, :] = to_sqnp(compute_similarities(\n",
    "        vals[i], vals, agent.dnd.kernel))\n",
    "\n",
    "# plot\n",
    "dmats = {'key': dmat_kk, 'value': dmat_vv}\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for i, (label_i, dmat_i) in enumerate(dmats.items()):\n",
    "    sns.heatmap(dmat_i, cmap='viridis', square=True, ax=axes[i])\n",
    "    axes[i].set_xlabel(f'id, {label_i} i')\n",
    "    axes[i].set_ylabel(f'id, {label_i} j')\n",
    "    axes[i].set_title(\n",
    "        f'{label_i}-{label_i} similarity, metric = {agent.dnd.kernel}'\n",
    "    )\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''project memory content to low dim space'''\n",
    "\n",
    "# convert the values to a np array, #memories x mem_dim\n",
    "vals_np = np.vstack([to_sqnp(vals[i]) for i in range(n_mems)])\n",
    "# project to PC space\n",
    "vals_centered = (vals_np - np.mean(vals_np, axis=0, keepdims=True))\n",
    "U, S, _ = np.linalg.svd(vals_centered, full_matrices=False)\n",
    "vals_pc = np.dot(U, np.diag(S))\n",
    "\n",
    "# pick pcs\n",
    "pc_x = 0\n",
    "pc_y = 1\n",
    "\n",
    "# plot\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "Y_phase2 = to_sqnp(Y[:n_unique_example, 0])\n",
    "for y_val in np.unique(Y_phase2):\n",
    "    ax.scatter(\n",
    "        vals_pc[Y_phase2 == y_val, pc_x],\n",
    "        vals_pc[Y_phase2 == y_val, pc_y],\n",
    "        marker='o', alpha=.7,\n",
    "    )\n",
    "ax.set_title(f'Each point is a memory (i.e. value)')\n",
    "ax.set_xlabel(f'PC {pc_x}')\n",
    "ax.set_ylabel(f'PC {pc_y}')\n",
    "ax.legend(['left trial', 'right trial'], bbox_to_anchor=(.6, .3))\n",
    "sns.despine(offset=20)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
